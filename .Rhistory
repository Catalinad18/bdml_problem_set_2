tune_grid(resamples = block_folds,
grid = lambda_grid_lasso,
metrics = metric_set(mae),
control=control_grid(verbose=TRUE))
results_tuning_lasso <- tune_result %>%
collect_metrics()
tune_result %>%
collect_metrics() %>%
ggplot(aes(penalty, mean, color = .metric)) +
geom_line(size = 1.5) +
scale_x_log10() +
theme(legend.position = "none") +
labs(title = "RMSE")
#Best fit
tune_best <- tune_result %>%
select_best(metric = "mae")
best_lasso <- linear_reg(penalty = tune_best$penalty, mixture = 1) %>%
set_engine("glmnet")
best_lasso_wflow <-
workflow() %>%
add_recipe(lasso_rec) %>%
add_model(best_lasso)
best_lasso_fit <-
best_lasso_wflow %>%
fit(new_train)
## Es interesante ver las variables que quedaron seleccionadas
coefs_lasso <- best_lasso_fit %>%
pull_workflow_fit() %>%
tidy()
View(coefs_lasso)
write_csv(coefs_lasso, "stores/Bases Finales/coefs_lasso.csv")
coefs_lasso <- read_csv("stores/Bases Finales/coefs_lasso.csv")
View(coefs_lasso)
selected_vars <- coefs_lasso$term[coefs_lasso$estimate!=0]
#Coefs Lasso para seleccionar variables
coefs_lasso <- read_csv("stores/Bases Finales/coefs_lasso.csv")
selected_vars <- coefs_lasso$term[coefs_lasso$estimate!=0][2:length(selected_vars)]
##############################################################
#       Big Data y Machine Learning                          #
#  Taller 2: Ãrboles, Bagging, Random Forests y  Boosting    #
##############################################################
# Cargar datos y eliminar columnas que no son predictores ------------------------------------------------------------
# Clean environment
rm(list = ls())
#-------------------------------------------
require(pacman)
p_load(tidymodels,
caret,
rpart,
modeldata,
spatialsample,
tidyverse,
rio,
leaflet,
randomForest,
rattle,
spatialsample)
# Datos -------------------------------------------------------------------
sp_data <- readRDS("stores/Bases Finales/sp_data_final.rds")
dense_dtm_train <- readRDS("stores/Bases Finales/DTM_train.rds")
dense_dtm_test <- readRDS("stores/Bases Finales/DTM_test.rds")
new_data <- sp_data %>%
mutate(log_price=log(price)) %>%
select(-c(price, city, month, year, title, description, piso_info, area,
precio_por_mt2, area, property_type_2,
geometry, operation_type)) %>%
mutate(across(
.cols = starts_with('dist'),
.fns = list(sq = function(x) {x^2},
cube = function(x) {x^3}),
.names = "{.col}_{.fn}")) %>%
mutate(across(c(surface_covered, bedrooms, bathrooms),
.fns = list(sq = function(x) {x^2},
cube = function(x) {x^3}),
.names = "{.col}_{.fn}")) %>%
mutate(ESTRATO=as.character(ESTRATO))
new_train<- new_data %>%
filter(type=="train") %>%
select(-c(type, property_id)) %>%
bind_cols(dense_dtm_train)
new_test<- new_data %>%
filter(type=="test") %>%
select(-c(type, property_id)) %>%
bind_cols(dense_dtm_test)
#Coefs Lasso para seleccionar variables
coefs_lasso <- read_csv("stores/Bases Finales/coefs_lasso.csv")
selected_vars <- coefs_lasso$term[coefs_lasso$estimate!=0][2:length(selected_vars)]
selected_vars <- coefs_lasso$term[coefs_lasso$estimate!=0]
selected_vars <- selected_vars[2:length(selected_vars)]
coefs_lasso <- read_csv("stores/Bases Finales/coefs_lasso.csv")
selected_vars <- coefs_lasso$term[coefs_lasso$estimate!=0]
selected_vars <- selected_vars[2:length(selected_vars)]
# Folds -------------------------------------------------------------------
set.seed(123)
block_folds <- spatial_block_cv(new_train, v = 10)
autoplot(block_folds)
#RECETA PARA LOS MODELOS --------------------------------------------------
#Recipe
#Con todas las variables
rec_trees_forests_boost <-
recipe(log_price ~ ., data = select(as_tibble(new_train), -geometry)) %>%
step_dummy(ESTRATO) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
step_normalize(all_numeric_predictors(), -all_outcomes())
#Quitando lasso
rec_trees_forests_boost_select <-
recipe(log_price ~ ., data = select(as_tibble(new_train), -geometry)) %>%
step_dummy(ESTRATO) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
step_rm(all_of(selected_vars)) %>%
step_normalize(all_numeric_predictors(), -all_outcomes())
#Model
tree <- decision_tree(
tree_depth = tune(),
min_n = tune()
) %>%
set_mode("regression")
tune_grid_tree <- grid_regular(
tree_depth(),
min_n(),
levels = c(10, 10)
)
#Con todas las variables
rec_trees_forests_boost <-
recipe(log_price ~ ., data = select(as_tibble(new_train), -geometry)) %>%
step_dummy(ESTRATO) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
step_normalize(all_numeric_predictors(), -all_outcomes())
#Quitando lasso
rec_trees_forests_boost_select <-
recipe(log_price ~ ., data = select(as_tibble(new_train), -geometry)) %>%
step_dummy(ESTRATO) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
step_rm(all_of(selected_vars)) %>%
step_normalize(all_numeric_predictors(), -all_outcomes())
# CART --------------------------------------------------------------------
#Model
tree <- decision_tree(
tree_depth = tune(),
min_n = tune()
) %>%
set_mode("regression")
tune_grid_tree <- grid_regular(
tree_depth(),
min_n(),
levels = c(10, 10)
)
#Workflows
workflow_tree <- workflow() %>%
add_recipe(rec_trees_forests_boost) %>%
add_model(tree)
workflow_tree_lasso <- workflow() %>%
add_recipe(rec_trees_forests_boost_select) %>%
add_model(tree)
#Tune grid
tune_tree <- workflow_tree %>%
tune_grid(resamples = block_folds,
grid = tune_grid_tree,
metrics = metric_set(mae),
control=control_grid(verbose=TRUE))
# Cargar datos y eliminar columnas que no son predictores ------------------------------------------------------------
# Clean environment
rm(list = ls())
#-------------------------------------------
require(pacman)
p_load(tidymodels,
caret,
rpart,
modeldata,
spatialsample,
tidyverse,
rio,
leaflet,
randomForest,
rattle,
spatialsample)
# Datos -------------------------------------------------------------------
sp_data <- readRDS("stores/Bases Finales/sp_data_final.rds")
dense_dtm_train <- readRDS("stores/Bases Finales/DTM_train.rds")
dense_dtm_test <- readRDS("stores/Bases Finales/DTM_test.rds")
new_data <- sp_data %>%
mutate(log_price=log(price)) %>%
select(-c(price, city, month, year, title, description, piso_info, area,
precio_por_mt2, area, property_type_2,
geometry, operation_type)) %>%
mutate(across(
.cols = starts_with('dist'),
.fns = list(sq = function(x) {x^2},
cube = function(x) {x^3}),
.names = "{.col}_{.fn}")) %>%
mutate(across(c(surface_covered, bedrooms, bathrooms),
.fns = list(sq = function(x) {x^2},
cube = function(x) {x^3}),
.names = "{.col}_{.fn}")) %>%
mutate(ESTRATO=as.character(ESTRATO))
new_train<- new_data %>%
filter(type=="train") %>%
select(-c(type, property_id)) %>%
bind_cols(dense_dtm_train)
new_test<- new_data %>%
filter(type=="test") %>%
select(-c(type, property_id)) %>%
bind_cols(dense_dtm_test)
#Coefs Lasso para seleccionar variables
coefs_lasso <- read_csv("stores/Bases Finales/coefs_lasso.csv")
selected_vars <- coefs_lasso$term[coefs_lasso$estimate!=0]
selected_vars <- selected_vars[2:length(selected_vars)]
# Folds -------------------------------------------------------------------
set.seed(123)
block_folds <- spatial_block_cv(new_train, v = 5)
autoplot(block_folds)
#RECETA PARA LOS MODELOS --------------------------------------------------
#Recipe
#Con todas las variables
rec_trees_forests_boost <-
recipe(log_price ~ ., data = select(as_tibble(new_train), -geometry)) %>%
step_dummy(ESTRATO) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
step_normalize(all_numeric_predictors(), -all_outcomes())
#Quitando lasso
rec_trees_forests_boost_select <-
recipe(log_price ~ ., data = select(as_tibble(new_train), -geometry)) %>%
step_dummy(ESTRATO) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
step_rm(all_of(selected_vars)) %>%
step_normalize(all_numeric_predictors(), -all_outcomes())
# CART --------------------------------------------------------------------
#Model
tree <- decision_tree(
tree_depth = tune(),
min_n = tune()
) %>%
set_mode("regression")
tune_grid_tree <- grid_regular(
tree_depth(),
min_n(),
levels = c(10, 5)
)
#Workflows
workflow_tree <- workflow() %>%
add_recipe(rec_trees_forests_boost) %>%
add_model(tree)
workflow_tree_lasso <- workflow() %>%
add_recipe(rec_trees_forests_boost_select) %>%
add_model(tree)
#Tune grid
tune_tree <- workflow_tree %>%
tune_grid(resamples = block_folds,
grid = tune_grid_tree,
metrics = metric_set(mae),
control=control_grid(verbose=TRUE))
results_tuning_tree <- tune_tree %>%
collect_metrics()
tune_tree_lasso <- workflow_tree_lasso %>%
tune_grid(resamples = block_folds,
grid = tune_grid_tree,
metrics = metric_set(mae),
control=control_grid(verbose=TRUE))
results_tuning_tree_lasso <- tune_tree_lasso %>%
collect_metrics()
new_data <- sp_data %>%
mutate(log_price=log(price)) %>%
select(-c(price, city, month, year, title, description, piso_info, area,
precio_por_mt2, area, property_type_2,
geometry, operation_type)) %>%
mutate(across(
.cols = starts_with('dist'),
.fns = list(sq = function(x) {x^2},
cube = function(x) {x^3}),
.names = "{.col}_{.fn}")) %>%
mutate(across(c(surface_covered, bedrooms, bathrooms),
.fns = list(sq = function(x) {x^2},
cube = function(x) {x^3}),
.names = "{.col}_{.fn}")) %>%
mutate(ESTRATO=as.character(ESTRATO))
new_train<- new_data %>%
filter(type=="train") %>%
select(-c(type, property_id)) %>%
bind_cols(dense_dtm_train)
new_test<- new_data %>%
filter(type=="test") %>%
select(-c(type, property_id)) %>%
bind_cols(dense_dtm_test)
View(new_data)
selected_vars[selected_vars=="property_type"]
selected_vars[selected_vars=="property_type2.casa"]
selected_vars[selected_vars=="property_type2_Casa"]
selected_vars[selected_vars=="property_type_2_Casa"]
selected_vars <- coefs_lasso$term[coefs_lasso$estimate==0]
selected_vars <- selected_vars[2:length(selected_vars)]
selected_vars <- coefs_lasso$term[coefs_lasso$estimate==0]
drop_vars <-selected_vars[selected_vars!="property_type"]
"property_type"%in%drop_vars
selected_vars <- coefs_lasso$term[coefs_lasso$estimate==0]
#Quitando lasso
rec_trees_forests_boost_select <-
recipe(log_price ~ ., data = select(as_tibble(new_train), -geometry)) %>%
step_dummy(ESTRATO) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
step_rm(all_of(selected_vars)) %>%
step_normalize(all_numeric_predictors(), -all_outcomes())
tune_tree_lasso <- workflow_tree_lasso %>%
tune_grid(resamples = block_folds,
grid = tune_grid_tree,
metrics = metric_set(mae),
control=control_grid(verbose=TRUE))
results_tuning_tree_lasso <- tune_tree_lasso %>%
collect_metrics()
#Best Fit, Training and K-validation
best_tree <- select_best(tune_tree, metric = "mae")
best_tree_lasso <- select_best(tune_tree_lasso, metric = "mae")
#Best Fit, Training and K-validation
best_tree <- select_best(tune_tree, metric = "mae")
best_tree_lasso <- select_best(tune_tree_lasso, metric = "mae")
#Best Fit, Training and K-validation
best_tree <- select_best(tune_tree, metric = "mae")
best_tree_lasso <- select_best(tune_tree_lasso, metric = "mae")
#Best Fit, Training and K-validation
best_tree <- select_best(tune_tree, metric = "mae")
best_tree_lasso <- select_best(tune_tree_lasso, metric = "mae")
#Best Fit, Training and K-validation
best_tree <- select_best(tune_tree, metric = "mae")
best_tree_lasso <- select_best(tune_tree_lasso, metric = "mae")
#Best Fit, Training and K-validation
best_tree <- select_best(tune_tree, metric = "mae")
best_tree_lasso <- select_best(tune_tree_lasso, metric = "mae")
#Best Fit, Training and K-validation
best_tree <- select_best(tune_tree, metric = "mae")
best_tree_lasso <- select_best(tune_tree_lasso, metric = "mae")
1+1
# Clean environment
rm(list = ls())
#-------------------------------------------
require(pacman)
p_load(tidymodels,
caret,
rpart,
modeldata,
spatialsample,
tidyverse,
rio,
leaflet,
randomForest,
rattle,
spatialsample,
doParallel)
# Datos -------------------------------------------------------------------
sp_data <- readRDS("stores/Bases Finales/sp_data_final.rds")
dense_dtm_train <- readRDS("stores/Bases Finales/DTM_train.rds")
dense_dtm_test <- readRDS("stores/Bases Finales/DTM_test.rds")
new_data <- sp_data %>%
mutate(log_price=log(price)) %>%
select(-c(price, city, month, year, title, description, piso_info, area,
precio_por_mt2, area, property_type_2,
geometry, operation_type)) %>%
mutate(across(
.cols = starts_with('dist'),
.fns = list(sq = function(x) {x^2},
cube = function(x) {x^3}),
.names = "{.col}_{.fn}")) %>%
mutate(across(c(surface_covered, bedrooms, bathrooms),
.fns = list(sq = function(x) {x^2},
cube = function(x) {x^3}),
.names = "{.col}_{.fn}")) %>%
mutate(ESTRATO=as.character(ESTRATO))
new_train<- new_data %>%
filter(type=="train") %>%
select(-c(type, property_id)) %>%
bind_cols(dense_dtm_train)
new_test<- new_data %>%
filter(type=="test") %>%
select(-c(type, property_id)) %>%
bind_cols(dense_dtm_test)
#Coefs Lasso para seleccionar variables
coefs_lasso <- read_csv("stores/Bases Finales/coefs_lasso.csv")
selected_vars <- coefs_lasso$term[coefs_lasso$estimate==0]
set.seed(123)
block_folds <- spatial_block_cv(new_train, v = 5)
autoplot(block_folds)
#Recipe
#Con todas las variables
rec_trees_forests_boost <-
recipe(log_price ~ ., data = select(as_tibble(new_train), -geometry)) %>%
step_dummy(ESTRATO) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
step_normalize(all_numeric_predictors(), -all_outcomes())
#Quitando lasso
rec_trees_forests_boost_select <-
recipe(log_price ~ ., data = select(as_tibble(new_train), -geometry)) %>%
step_dummy(ESTRATO) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
step_normalize(all_numeric_predictors(), -all_outcomes()) %>%
step_rm(all_of(selected_vars))
#Model
random_forest <- rand_forest(
mtry = tune(), #default para probar. Usualmente lleva al optimo
min_n = tune(),
trees = tune(),
) %>%
set_engine("ranger") %>%
set_mode("regression")
random_forest_grid <- grid_regular(mtry(range = c(26, 500)),
min_n(),
trees(range = c(1000, 2000)),
levels = c(10,5,5))
#Workflow
workflow_random_forest <- workflow() %>%
add_recipe(rec_trees_forests_boost_select) %>%
add_model(random_forest)
# RANDOM FORESTS--------------------------------------------------------------
set.seed(123)
#Model
random_forest <- rand_forest(
mtry = tune(), #default para probar. Usualmente lleva al optimo
min_n = tune(),
trees = tune(),
) %>%
set_engine("ranger") %>%
set_mode("regression")
random_forest_grid <- grid_regular(mtry(range = c(26, 500)),
min_n(),
trees(range = c(1000, 2000)),
levels = c(10,5,5))
#Workflow
workflow_random_forest <- workflow() %>%
add_recipe(rec_trees_forests_boost_select) %>%
add_model(random_forest)
#Tune grid
registerDoParallel(cores = 8)
tune_random_forest <- workflow_random_forest %>%
tune_grid(resamples = block_folds,
grid = random_forest_grid,
metrics = metric_set(mae),
control=control_grid(verbose=TRUE))
require(pacman)
p_load(tidymodels,
caret,
rpart,
modeldata,
spatialsample,
tidyverse,
rio,
leaflet,
randomForest,
rattle,
spatialsample,
doParallel,
ranger)
# Datos -------------------------------------------------------------------
sp_data <- readRDS("stores/Bases Finales/sp_data_final.rds")
dense_dtm_train <- readRDS("stores/Bases Finales/DTM_train.rds")
dense_dtm_test <- readRDS("stores/Bases Finales/DTM_test.rds")
new_data <- sp_data %>%
mutate(log_price=log(price)) %>%
select(-c(price, city, month, year, title, description, piso_info, area,
precio_por_mt2, area, property_type_2,
geometry, operation_type)) %>%
mutate(across(
.cols = starts_with('dist'),
.fns = list(sq = function(x) {x^2},
cube = function(x) {x^3}),
.names = "{.col}_{.fn}")) %>%
mutate(across(c(surface_covered, bedrooms, bathrooms),
.fns = list(sq = function(x) {x^2},
cube = function(x) {x^3}),
.names = "{.col}_{.fn}")) %>%
mutate(ESTRATO=as.character(ESTRATO))
new_train<- new_data %>%
filter(type=="train") %>%
select(-c(type, property_id)) %>%
bind_cols(dense_dtm_train)
new_test<- new_data %>%
filter(type=="test") %>%
select(-c(type, property_id)) %>%
bind_cols(dense_dtm_test)
#Coefs Lasso para seleccionar variables
coefs_lasso <- read_csv("stores/Bases Finales/coefs_lasso.csv")
selected_vars <- coefs_lasso$term[coefs_lasso$estimate==0]
# RANDOM FORESTS--------------------------------------------------------------
set.seed(123)
#Model
random_forest <- rand_forest(
mtry = tune(), #default para probar. Usualmente lleva al optimo
min_n = tune(),
trees = tune(),
) %>%
set_engine("ranger") %>%
set_mode("regression")
random_forest_grid <- grid_regular(mtry(range = c(26, 500)),
min_n(),
trees(range = c(1000, 2000)),
levels = c(10,5,5))
#Workflow
workflow_random_forest <- workflow() %>%
add_recipe(rec_trees_forests_boost_select) %>%
add_model(random_forest)
#Tune grid
registerDoParallel(cores = 8)
tune_random_forest <- workflow_random_forest %>%
tune_grid(resamples = block_folds,
grid = random_forest_grid,
metrics = metric_set(mae),
control=control_grid(verbose=TRUE))
