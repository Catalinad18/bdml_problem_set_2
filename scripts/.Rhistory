type = "text", min.max = TRUE, mean.sd = TRUE,
nobs = TRUE, median = TRUE, iqr = FALSE,
digits = 1, align = T,
title = "Summary Statistics"
)
p_load(stargazer)
estadisticas_descriptivas <- stargazer(tabla_final_limpia,
type = "text", min.max = TRUE, mean.sd = TRUE,
nobs = TRUE, median = TRUE, iqr = FALSE,
digits = 1, align = T,
title = "Summary Statistics"
)
salario_horas <- tabla_final_limpia$y_ingLab_m_ha
atipico <- which.max(salario_horas)
salario_horas[atipico] <- NA
estadisticas_descriptivas <- stargazer(tabla_final_limpia,
type = "text", min.max = TRUE, mean.sd = TRUE,
nobs = TRUE, median = TRUE, iqr = FALSE,
digits = 1, align = T,
title = "Summary Statistics"
)
salario_horas <- tabla_final_limpia$y_ingLab_m_ha
atipico <- which.max(tabla_final_limpia$y_ingLab_m_ha)
tabla_final_limpia$y_ingLab_m_ha[atipico] <- NA
estadisticas_descriptivas <- stargazer(tabla_final_limpia,
type = "text", min.max = TRUE, mean.sd = TRUE,
nobs = TRUE, median = TRUE, iqr = FALSE,
digits = 1, align = T,
title = "Summary Statistics"
)
estadisticas_descriptivas <- stargazer(tabla_final_limpia,
type = "text", min.max = TRUE, mean.sd = TRUE,
nobs = TRUE, median = TRUE, mode = TRUE, iqr = FALSE,
digits = 1, align = T,
title = "Summary Statistics"
)
install.packages(“modeest”)
p_load()
p_load(modeest)
moda <- mfv(tabla_final_limpia$y_ingLab_m_ha)
moda
View(tabla_final_limpia)
db <- import("https://raw.githubusercontent.com/ignaciomsarmiento/datasets/main/house_priceCALI.rds")
library(pacman)
p_load(tidyverse, # Manipular dataframes
rio, # Import data easily
plotly, # Gráficos interactivos
leaflet, # Mapas interactivos
rgeos, # Calcular centroides de un poligono
tmaptools, # geocode_OSM()
sf, # Leer/escribir/manipular datos espaciales
osmdata, # Get OSM's data
tidymodels) #para modelos de ML
# Importamos datos
db <- import("https://raw.githubusercontent.com/ignaciomsarmiento/datasets/main/house_priceCALI.rds")
dim(db)
db %>%
count(property_type)
db <- db %>%
filter(property_type == "Apartamento" | property_type == "Casa")
dim(db)
db %>%
summarise_all(~sum(is.na(.))) %>% transpose()
#Encontremos la moda de número de habitaciones, cuartos y número de baños
db %>%
count(rooms) %>% head()
db %>%
count(bedrooms)
## # A tibble: 24 × 2
##    bedrooms     n
##       <int> <int>
##  1        0     2
##  2        1   220
##  3        2  1877
##  4        3  7485
##  5        4  2452
##  6        5  1050
##  7        6   644
##  8        7   354
##  9        8   245
## 10        9   137
## # ℹ 14 more rows
db %>%
count(bathrooms)
# Calcular la mediana
mediana_sup_cubierta <- median(db$surface_covered, na.rm = TRUE)
mediana_sup_total<- median(db$surface_total, na.rm = TRUE)
# Imputar datos faltantes
db <- db %>%
mutate(rooms = replace_na(rooms, 3),
bedrooms = replace_na(bedrooms, 3),
bathrooms = replace_na(bathrooms, 2),
surface_covered = replace_na(surface_covered, mediana_sup_cubierta),
surface_total = replace_na(surface_total, mediana_sup_total),)
# Ahora veamos la distribución del precio de los immuebles
summary(db$price) %>%
as.matrix() %>%
as.data.frame() %>%
mutate(V1 = scales::dollar(V1))
# Calculamos valor del metro cuadrado
db <- db %>%
mutate(precio_por_mt2 = round(price / surface_total, 0))
summary(db$precio_por_mt2) %>%
as.matrix() %>%
as.data.frame() %>%
mutate(V1 = scales::dollar(V1))
# Calculamos valor del metro cuadrado
db <- db %>%
mutate(precio_por_mt2 = round(price / surface_total, 0))
summary(db$precio_por_mt2) %>%
as.matrix() %>%
as.data.frame() %>%
mutate(V1 = scales::dollar(V1))
#Filtramos ese outlier que resulta no ser real
db <- db %>%
filter(between(precio_por_mt2, 100000,  30e6))
# Visualicemos la distribución de nuestra variable de interés
p <- ggplot(db, aes(x = price)) +
geom_histogram(fill = "darkblue", alpha = 0.4) +
labs(x = "Valor de venta (log-scale)", y = "Cantidad") +
scale_x_log10(labels = scales::dollar) +
theme_bw()
ggplotly(p)
# Observamos la primera visualización
leaflet() %>%
addTiles() %>%
addCircles(lng = db$lon,
lat = db$lat)
# Ahora solo nos quedaremos con las observaciones que efectivamente están dentro de Cali y no están mal georeferenciadas
limites <- getbb("Cali Colombia")
db <- db %>%
filter(
between(lon, limites[1, "min"], limites[1, "max"]) &
between(lat, limites[2, "min"], limites[2, "max"])
db <- db %>%
db <- db %>%
mutate(precio_por_mt2_sc =( (precio_por_mt2 - min(precio_por_mt2)) / (max(precio_por_mt2) - min(precio_por_mt2))))
db <- db %>%
mutate(color = case_when(property_type == "Apartamento" ~ "#2A9D8F",
property_type == "Casa" ~ "#F4A261"))
# Vamos a crear un mensaje en popup con html
html <- paste0("<b>Precio:</b> ",
scales::dollar(db$price),
"<br> <b>Area:</b> ",
as.integer(db$surface_total), " mt2",
"<br> <b>Tipo de immueble:</b> ",
db$property_type,
"<br> <b>Numero de alcobas:</b> ",
as.integer(db$rooms),
"<br> <b>Numero de baños:</b> ",
as.integer(db$bathrooms),
"<br> <b>Sector:</b> ",
db$l4,
"<br> <b>Barrio:</b> ",
db$l5)
# Eliminamos los immuebles con área menor a 20
db <- db %>% filter( surface_covered > 20)
# Encontram,os el queremos que sea el centro del mapa
latitud_central <- mean(db$lat)
longitud_central <- mean(db$lon)
# Creamos el plot
leaflet() %>%
addTiles() %>%
setView(lng = longitud_central, lat = latitud_central, zoom = 12) %>%
addCircles(lng = db$lon,
lat = db$lat,
col = db$color,
fillOpacity = 1,
opacity = 1,
radius = db$precio_por_mt2_sc*10,
popup = html)
latitud_central <- mean(db$lat)
longitud_central <- mean(db$lon)
latitud_central <- mean(db$lat)
longitud_central <- mean(db$lon)
db$description[1]
db <- db %>%
mutate(parqueadero = as.numeric(grepl("parqueadero", db$description)))
require("pacman")
#Cargamos sf que será la librería a utilizar y tidyverse para nuestras necesidades de manejo de datos.
p_load(sf, tidyverse)
#Cargamos los datos de los colegios.
colegios <- read_sf("https://raw.githubusercontent.com/ignaciomsarmiento/datasets
/1585e2fef4eb7189af500f52a1022073d3ab0de0/colegios_bogota.json")
colegios <- read_sf("https://raw.githubusercontent.com/ignaciomsarmiento/datasets/1585e2fef4eb7189af500f52a1022073d3ab0de0/colegios_bogota.json")
head(colegios)
class(colegios)
ggplot()+
geom_sf(data=colegios)
ggplot()+
geom_sf(data=colegios) +
theme_bw()
ciclovia <- st_read("C:\Users\PC\Documents\Uniandes\Economía\Big Data y Machine Learning para Economía Aplicada\Semana 7\Ciclovia")
ciclovia <- st_read("C:/Users/PC/Documents/Uniandes/Economía/Big Data y Machine Learning para Economía Aplicada/Semana 7/Ciclovia")
ciclovia <- st_read("C:/Users/PC/Documents/Uniandes/Economía/Big Data y Machine Learning para Economía Aplicada/Semana 7/Ciclovia")
head(ciclovias)
ciclovias <- st_read("C:/Users/PC/Documents/Uniandes/Economía/Big Data y Machine Learning para Economía Aplicada/Semana 7/Ciclovia")
head(ciclovias)
ggplot()+
geom_sf(data=ciclovias) +
theme_bw() +
theme(axis.title =element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_text(size=6))
upla <- st_read("C:/Users/PC/Documents/Uniandes/Economía/Big Data y Machine Learning para Economía Aplicada/Semana 7/upla")
ggplot()+
geom_sf(data=upla) +
theme_bw() +
theme(axis.title =element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_text(size=6))
head(upla)
ggplot()+
geom_sf(data=upla, aes(fill = UPlArea)) +
theme_bw() +
theme(axis.title =element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_text(size=6))
ggplot()+
geom_sf(data=upla %>% filter(grepl("RIO",UPlNombre)==FALSE),size=.3,fill=NA) +
#filtramos usando expresiones regulares
geom_sf(data=colegios[1:250,],shape=18) + #utilizamos solo los primeros 250 col
egios y los representamos con romboides (shape=18)
ggplot()+
geom_sf(data=upla %>% filter(grepl("RIO",UPlNombre)==FALSE),size=.3,fill=NA) + #filtramos usando expresiones regulares
geom_sf(data=colegios[1:250,],shape=18) + #utilizamos solo los primeros 250 colegios y los representamos con romboides (shape=18)
geom_sf(data=ciclovias,col="red") +
theme_bw() +
theme(axis.title =element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_text(size=6))
st_crs(ciclovias)
st_crs(colegios) #MAGNA-SIRGAS
cilovias<-st_transform(ciclovias,4686)
st_crs(ciclovias)
db<-data.frame(place=c("Uniandes","Banco de La Republica"),lat=c(4.601590,4.60215
1), long=c(-74.066391,-74.072350), nudge_y=c(-0.001,0.001))
db<-data.frame(place=c("Uniandes","Banco de La Republica"),lat=c(4.601590,4.602151), long=c(-74.066391,-74.072350), nudge_y=c(-0.001,0.001))
db<-db %>% mutate(latp=lat,longp=long)
db<-st_as_sf(db,coords=c('longp','latp'),crs=4326)
db
db<-st_transform(db,4686)
st_crs(db)
ggplot()+
geom_sf(data=upla %>% filter(UPlNombre%in%c("LA CANDELARIA","LAS NIEVES")), fil
l = NA) +
ggplot()+
geom_sf(data=upla %>% filter(UPlNombre%in%c("LA CANDELARIA","LAS NIEVES")), fill = NA) +
geom_sf(data=db, col="red") +
geom_label(data = db, aes(x = long, y = lat, label = place),
size = 3, col = "black", fontface = "bold", nudge_y =db$nudge_y) +
theme_bw() +
theme(axis.title =element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_text(size=6))
st_distance(db)
ciclovias<-st_transform(ciclovias, 4686)
db<-st_transform(db, 4686)
st_distance(db,ciclovias)
Datos Espaciales. Fundamentos teóricos. https://bloqueneon.uniandes.edu.co//content/enforced/211813-UN_202...
ciclovias<-st_transform(ciclovias, 4686)
db<-st_transform(db, 4686)
st_distance(db,ciclovias)
ggplot()+
geom_sf(data=ciclovias[8,], fill = NA) +
geom_sf(data=db, col="red") +
theme_bw() +
theme(axis.title =element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_text(size=6))
upla<-st_transform(upla,crs=st_crs(db))
db<- st_join(db,upla,join=st_intersects)
head(db)
require("pacman")
p_load(tidyverse, sf, tmaptools) #La librería tmaptools contiene la función geocode_OSM() que se conecta a la API de OpenStreetMap. Este retorna las coordenadas geográ􀀁cas del sitio/dirección buscado.
geocode_OSM("Casa de Nariño, Bogotá")
require("pacman")
p_load(tidyverse, sf, tmaptools) #La librería tmaptools contiene la función geocode_OSM() que se conecta a la API de OpenStreetMap. Este retorna las coordenadas geográ􀀁cas del sitio/dirección buscado.
geocode_OSM("Casa de Nariño, Bogotá")
geocode_OSM("Casa Rosada, Buenos Aires",as.sf=TRUE)
p_load("osmdata")
available_features() %>% head(20)
available_tags("amenity") #Cada feature contiene una lista de tags , que describe un atributo geográ􀀁co de la feature seleccionada. Por ejemplo, podemos ver los tags disponibles para la feature amenity con la función available_tags()
require("pacman")
p_load(tidyverse, sf, tmaptools) #La librería tmaptools contiene la función geocode_OSM() que se conecta a la API de OpenStreetMap. Este retorna las coordenadas geográ􀀁cas del sitio/dirección buscado.
geocode_OSM("Casa de Nariño, Bogotá")
geocode_OSM("Casa Rosada, Buenos Aires",as.sf=TRUE)
p_load("osmdata")
available_features() %>% head(20)
available_tags("amenity") #Cada feature contiene una lista de tags , que describe un atributo geográ􀀁co de la feature seleccionada. Por ejemplo, podemos ver los tags disponibles para la feature amenity con la función available_tags()
bogota<-opq(bbox = getbb("Bogotá Colombia"))
bogota
universidades<- bogota %>%
add_osm_feature(key="amenity",value="university") %>% # de las amenities disponibles, seleccionamos las universidades
osmdata_sf() #transformamos a un objeto sf
puntos_universidades<-universidades$osm_point
head(puntos_universidades)
ggplot()+
geom_sf(data=puntos_universidades) +
theme_bw()
ggplot()+
geom_sf(data=puntos_universidades) +
theme_bw()
ML<-universidades$osm_polygons %>% filter(grepl("Mario Laserna" ,name))
ML
ggplot()+
geom_sf(data=ML) +
theme_bw()
p_load("leaflet")
leaflet() %>%
addTiles() %>%  #capa base
addPolygons(data=ML) #capa edificio ML
leaflet() %>%
addProviderTiles(providers$Stamen.Toner) %>%  #capa base
addCircles(data=puntos_universidades, popup = ~name) %>%  #agregamos puntos con un `pop up` que indica el nombre
addPolygons(data=ML) #capa edificio ML
install.packages("pdftools")
library(pdftools)
acuerdo_final <- pdf_text("C:/Users/PC/Documents/Uniandes/Economía/Big Data y Machine Learning para Economía Aplicada/Semana 7/acuerdo_final.pdf")
documento <- ""
for (pagina in 1:length(acuerdo_final)) {
documento <- paste(documento, acuerdo_final[pagina])
}
substr(documento, 1, 500)
library(stringi)
documento <- stri_trans_general(str = documento, id = "Latin-ASCII")
substr(documento, 1, 500)
documento <- gsub('[^A-Za-z0-9 ]+', ' ', documento)
substr(documento, 1, 1000)
# Minúsculas
documento <- tolower(documento)
# Espacios
documento <- gsub('\\s+', ' ', documento)
substr(documento, 1, 1000)
documento <- gsub("\\d+", "", documento)
documento <- gsub('\\s+', ' ', documento)
documento <- trimws(documento)
substr(documento, 1, 1000)
library(tokenizers)
install.packages("tokenizers")
library(tokenizers)
documento_tokenizado <- tokenize_words(documento)
length(unique(documento_tokenizado[[1]]))
lista_palabras1 <- stopwords(language = "es", source = "snowball")
lista_palabras1 <- stopwords(language = "es", source = "snowball")
lista_palabras <- union(lista_palabras1, lista_palabras2)
lista_palabras
install.packages("stopwords")
library(stopwords)
# Descargamos la lista de las stopwords en español de dos fuentes diferentes y las combinamos
lista_palabras1 <- stopwords(language = "es", source = "snowball")
lista_palabras2 <- stopwords(language = "es", source = "nltk")
lista_palabras <- union(lista_palabras1, lista_palabras2)
lista_palabras
documento_tokenizado <- documento_tokenizado[[1]]
n0 <- length(documento_tokenizado)
documento_tokenizado <- setdiff(documento_tokenizado, lista_palabras)
n1 <- length(documento_tokenizado)
n0 - n1
install.packages("SnowballC")
library(SnowballC)
library(SnowballC)
documento_tokenizado <- wordStem(documento_tokenizado, "spanish")
documento_tokenizado[1:500]
library(wordcloud)
install.packages("wordcloud")
library(wordcloud)
library(tidyverse)
frecuencia <- documento_tokenizado %>%
table() %>%
data.frame() %>%
rename("Palabra" = ".") %>%
arrange(desc(Freq))
set.seed(666)
png(filename = "wordcloud.png", width = 800, height = 800)
wordcloud(words = frecuencia$Palabra, freq = frecuencia$Freq, min.freq = 1,
max.words = 200, random.order=  FALSE, rot.per = 0.35,
colors = brewer.pal(8, "Dark2"))
dev.off()
require(pacman)
p_load(tidyverse,caret, gtsummary)
set.seed(12)
credit <- readRDS(url("https://github.com/ignaciomsarmiento/datasets/blob/main/credit_class.rds?raw=true"))
head(credit)
tbl_summary(credit, by = Default, statistic = list(all_continuous() ~ "{mean} ({sd})"))
credit<-credit %>% mutate(Default=factor(Default,levels=c(1,0),labels=c("Sí","No")))
credit = credit %>%
mutate(historygood  = ifelse(test = history == "good",
yes = 1,
no = 0))
credit = credit %>%
mutate(historypoor = ifelse(test = history == "poor",
yes = 1,
no = 0))
credit = credit %>%
mutate(purposeusedcar = ifelse(test = purpose == "usedcar",
yes = 1,
no = 0))
credit = credit %>%
mutate(purposegoods.repair = ifelse(test = purpose == "goods.repair",
yes = 1,
no = 0))
credit = credit %>%
mutate(purposeedu= ifelse(test = purpose == "edu",
yes = 1,
no = 0))
credit = credit %>%
mutate(foreigngerman= ifelse(test = foreign == "foreign",
yes = 1,
no = 0))
credit = credit %>%
mutate(rentTRUE= ifelse(test = rent == "TRUE",
yes = 1,
no = 0))
inTrain <- createDataPartition(
y = credit$Default,## La variable dependiente u objetivo
p = .7, ## Usamos 70%  de los datos en el conjunto de entrenamiento
list = FALSE)
training <- credit[ inTrain,]
testing  <- credit[-inTrain,]
nrow(training)
ctrl_def <- trainControl(
method = "cv",
summaryFunction = defaultSummary, #medida de rendimiento
number = 5) # número de folds
modelo_def <- train(Default ~ amount+installment+age+ historygood + historypoor + purposeusedcar+ purposegoods.repair + purposeedu + foreigngerman + rentTRUE,
data = training,
method = "glm", #for logit
trControl = ctrl_def,
family = "binomial",
preProcess = c("center", "scale")) # preprocesamiento de los datos
modelo_def
prediccion_def <- predict(modelo_def, newdata = testing)
confusionMatrix(prediccion_def, testing$Default)
control_two <- trainControl(method = "cv",
number = 5,
summaryFunction = twoClassSummary,
classProbs = TRUE)
modelo_two <- train( Default ~amount+installment+age+ historygood + historypoor + purposeusedcar+ purposegoods.repair + purposeedu + foreigngerman + rentTRUE,
data = training,
method = "glm", #for logit
trControl = control_two,
family = "binomial",
metric = 'ROC',
preProcess = c("center", "scale") ) # preprocesamiento de los datos
modelo_two
fiveStats <- function(...) c(twoClassSummary(...), defaultSummary(...))
ctrl<- trainControl(method = "cv",
number = 5,
summaryFunction = fiveStats,
classProbs = TRUE,
verbose=FALSE,
savePredictions = T)
modelo <- train(Default ~amount+installment+age+ historygood + historypoor + purposeusedcar+ purposegoods.repair + purposeedu + foreigngerman + rentTRUE,
data = training,
method = "glm", #for logit
trControl = ctrl,
family = "binomial",
metric = 'ROC',
preProcess = c("center", "scale") )
modelo
library(pacman)
p_load(tidyverse, tidymodels, glmnet)
df <- read_csv("../../stores/train.csv")
file_dir <- this.path::here()
setwd(file_dir)
df <- read_csv("../../stores/train.csv")
file_dir <- this.path::here()
setwd(file_dir)
df <- read_csv("../../stores/train.csv")
df_test <- read_csv(("../../stores/test.csv"))
df <- read_csv("../stores/train.csv")
df_test <- read_csv(("../stores/test.csv"))
glimpse(df)
glimpse(df_test)
sapply(df, function(x) sum(is.na(x)))
df$surface_total %>%
table(useNA = "ifany") %>%
prop.table() %>%
round(3)*100
df$surface_total %>%
table(useNA = "ifany") %>%
prop.table() %>%
round(3)*10
plot(density(df$surface_total))
plotDistribution = function (x) {
N = length(x)
x <- na.omit(x)
hist( x,col = "light blue",
probability = TRUE)
lines(density(x), col = "red", lwd = 3)
rug(x)
print(N-length(x))
}
ggplot(dat, aes(x=df$surface_total)) +
geom_histogram(binwidth=.5, colour="black", fill="white")
ggplot(df, aes(x=surface_total)) +
geom_histogram(binwidth=.5, colour="black", fill="white")
ggplot(df, aes(x=surface_total)) + geom_boxplot()
p_load(tidyverse, tidymodels, glmnet, stargazer)
descriptive_statistics <- stargazer(df,
type = "text", min.max = TRUE, mean.sd = TRUE,
nobs = TRUE, median = TRUE, iqr = FALSE,
digits = 1, align = T,
title = "Summary Statistics")
View(df)
